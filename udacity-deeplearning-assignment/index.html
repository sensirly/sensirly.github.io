<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://sensirly.github.io">
  <title>Udacity DeepLearning Assignment | 给时光以生命</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="deep learning入门公开课 课程链接的课堂作业，类似于TensorFlow的教学实践，总共分为三部分：首先是环境的搭建及TensorFlow的基本介绍；其次是手写字母识别（比手写数字识别数据集MNIST要困难一点），前后使用了线性模型、NN、DNN、CNN模型。最后两个任务是在维基百科数据集上做词向量表示和序列学习，分别使用了word2vec和LSTM算法。TensorFlow是个通用性">
<meta property="og:type" content="article">
<meta property="og:title" content="Udacity DeepLearning Assignment">
<meta property="og:url" content="http://sensirly.github.io/udacity-deeplearning-assignment/index.html">
<meta property="og:site_name" content="给时光以生命">
<meta property="og:description" content="deep learning入门公开课 课程链接的课堂作业，类似于TensorFlow的教学实践，总共分为三部分：首先是环境的搭建及TensorFlow的基本介绍；其次是手写字母识别（比手写数字识别数据集MNIST要困难一点），前后使用了线性模型、NN、DNN、CNN模型。最后两个任务是在维基百科数据集上做词向量表示和序列学习，分别使用了word2vec和LSTM算法。TensorFlow是个通用性">
<meta property="og:image" content="http://sensirly.github.io/../img/udacity-deep-learning/lenet-5.png">
<meta property="og:image" content="http://sensirly.github.io/../img/udacity-deep-learning/skip-gram-train.png">
<meta property="og:image" content="http://sensirly.github.io/../img/udacity-deep-learning/tsne-skip-gram.png">
<meta property="og:image" content="http://sensirly.github.io/../img/udacity-deep-learning/cbow-train.png">
<meta property="og:image" content="http://sensirly.github.io/../img/udacity-deep-learning/tsne-cbow.png">
<meta property="og:updated_time" content="2017-03-19T14:42:23.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Udacity DeepLearning Assignment">
<meta name="twitter:description" content="deep learning入门公开课 课程链接的课堂作业，类似于TensorFlow的教学实践，总共分为三部分：首先是环境的搭建及TensorFlow的基本介绍；其次是手写字母识别（比手写数字识别数据集MNIST要困难一点），前后使用了线性模型、NN、DNN、CNN模型。最后两个任务是在维基百科数据集上做词向量表示和序列学习，分别使用了word2vec和LSTM算法。TensorFlow是个通用性">
<meta name="twitter:image" content="http://sensirly.github.io/../img/udacity-deep-learning/lenet-5.png">
  
    <link rel="alternative" href="/atom.xml" title="给时光以生命" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  <link rel="stylesheet" href="/main.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-74191141-2', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/img/wangzai.jpeg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">Eric Li</a></h1>
		</hgroup>

		
		<p class="header-subtitle">学习使我感到充实而快乐，在此记录并分享这份喜悦</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/tags/machine-learning">机器学习</a></li>
	        
				<li><a href="/tags/reading">读书笔记</a></li>
	        
				<li><a href="/tags/online-course">在线课程</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">友链</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="facebook" target="_blank" href="https://www.facebook.com/kklisen" title="facebook">facebook</a>
		        
					<a class="mail" target="_blank" href="mailto:sdulisen@163.com" title="mail">mail</a>
		        
					<a class="weibo" target="_blank" href="http://weibo.com/sleric" title="weibo">weibo</a>
		        
					<a class="linkedin" target="_blank" href="https://www.linkedin.com/in/lisenlisen" title="linkedin">linkedin</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">Eric Li</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/img/wangzai.jpeg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">Eric Li</h1>
			</hgroup>
			
			<p class="header-subtitle">学习使我感到充实而快乐，在此记录并分享这份喜悦</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/tags/machine-learning">机器学习</a></li>
		        
					<li><a href="/tags/reading">读书笔记</a></li>
		        
					<li><a href="/tags/online-course">在线课程</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="facebook" target="_blank" href="https://www.facebook.com/kklisen" title="facebook">facebook</a>
			        
						<a class="mail" target="_blank" href="mailto:sdulisen@163.com" title="mail">mail</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/sleric" title="weibo">weibo</a>
			        
						<a class="linkedin" target="_blank" href="https://www.linkedin.com/in/lisenlisen" title="linkedin">linkedin</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        <article id="post-udacity-deeplearning-assignment" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Udacity DeepLearning Assignment
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>deep learning入门公开课 <a href="https://classroom.udacity.com/courses/ud730" target="_blank" rel="external">课程链接</a>的课堂作业，类似于TensorFlow的教学实践，总共分为三部分：首先是环境的搭建及TensorFlow的基本介绍；其次是手写字母识别（比手写数字识别数据集MNIST要困难一点），前后使用了线性模型、NN、DNN、CNN模型。最后两个任务是在维基百科数据集上做词向量表示和序列学习，分别使用了word2vec和LSTM算法。TensorFlow是个通用性很高的框架，通过图的形式定义网络结构及运算流程非常清晰易懂，提供的算子也越来越丰富，感谢前人造的轮子。<br>这里简单记录下完成作业过程中遇到的一些问题，前期还是比较坎坷的，后面就比较通畅了，详细代码存放在我的<a href="https://github.com/sensirly/udacity-deep-learning-assignment" target="_blank" rel="external">Github项目</a>中。<br><a id="more"></a></p>
<h2 id="0-环境搭建"><a href="#0-环境搭建" class="headerlink" title="0.环境搭建"></a>0.环境搭建</h2><ol>
<li>安装Homebrew、python、pip、docker、jupyter notebook</li>
<li>尝试了docker和pip两种方式安装TensorFlow。docker library的网站被墙了，可以直接在<a href="http://7xlgth.com1.z0.glb.clouddn.com/tensorflow.tar" target="_blank" rel="external">某网站</a>下载了镜像然后load到docker中,也可以按照readme里的流程使用教学docker镜像（ipynb文件同时下好，貌似也会被墙）。最后选择了直接本地安装运行  </li>
<li>使用pip安装scipy、sklearn时会特别慢，换成豆瓣的源后速度特别给力。<code>pip install sklearn -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code>酱紫</li>
<li>Assignment的要求以ipynb文件的形式放在<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity" target="_blank" rel="external">GitHub</a>上，可以通过<a href="https://www.tensorflow.org/tutorials/" target="_blank" rel="external">官方文档</a>或者<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/" target="_blank" rel="external">极客学院翻译的TensorFlow 官方文档中文版</a>了解一下TensorFlow的基本概念及原理：   <ul>
<li>使用<strong>tensor</strong>表示数据.在Python中,返回的tensor是<code>numpy ndarray</code>对象</li>
<li>使用图(<strong>graph</strong>)来表示计算任务.图中的节点被称之为<strong>op</strong>(operation). 一个op获得0个或多个Tensor执行计算, 产生0个或多个Tensor</li>
<li>在被称之为会话(<strong>Session</strong>)的上下文中执行图.会话将图的op分发到诸如CPU或GPU之类的设备上,同时提供执行op的方法并返回产生的Tensor</li>
<li>通过变量(<strong>Variable</strong>)维护状态(计数器等)</li>
<li>TensorFlow还提供了<strong>feed</strong>机制, 该机制可以临时替代图中的任意操作中的tensor可以对图中任何操作提交补丁,直接插入一个tensor</li>
</ul>
</li>
</ol>
<h2 id="1-notMNIST"><a href="#1-notMNIST" class="headerlink" title="1.notMNIST"></a>1.notMNIST</h2><p>notMNIST是升级版的MNIST，含有A-J10个类别的艺术印刷体字符，难度大于手写数字识别。这个assignment主要用于处理数据。  </p>
<ul>
<li>下载：assignment里提供的地址挂了，可以在<a href="http://yaroslavvb.com/upload/notMNIST/" target="_blank" rel="external">官网</a>上直接下载。   </li>
<li>格式转换：解压后判断每张图片的尺寸是否合规，剔除不合法图片,然后将数据转化成3维数组的形式存储。  </li>
<li>归一化：提供的代码对像素值进行了归一化，但是新下载的数据貌似已经做过归一化了，所以重复归一化会导致所有像素值几乎相同无法进行后续学习（准确率一直是10%，坑了好久），这个要根据解析后的标准差确认一下（之前的标准差只有0.01，对此产生了一点怀疑，然后顺藤摸瓜找到这个问题）。  </li>
<li>验证：使用pyplot将图像展示出来，参见<a href="http://matplotlib.org/users/pyplot_tutorial.html" target="_blank" rel="external">Pyplot tutorial</a>   </li>
<li>存储：将数据集随机划分为训练、测试、验证集，以pickle文件的形式进行存储。pickle模块将对象转化为文件保存在磁盘上，在需要的时候再读取并还原。   </li>
<li>训练：使用Logistic Regression做简单的训练，观察不同训练样本数量对预测准确度的影响。</li>
</ul>
<h2 id="2-fullyConnected"><a href="#2-fullyConnected" class="headerlink" title="2.fullyConnected"></a>2.fullyConnected</h2><p>样例中给出了如何使用TensorFlow构建一个多分类的逻辑回归：首先在graph中定义计算过程，然后在session中运行这些op。然后我们参照这个流程构造一层的NN网络，使用ReLu作为激活函数。使用miniBatch随机梯度进行训练，因此训练数据不是一个特定的值，而是占位符<strong>placeholder</strong>，在TensorFlow运行计算时通过feed机制输入这个值。使用batch训练的LR测试集准确率为85.9%，1-layer NN达到91.1%</p>
<h2 id="3-Regularization"><a href="#3-Regularization" class="headerlink" title="3. Regularization"></a>3. Regularization</h2><ul>
<li>Problem 1：分别对之前训练的LR和NN模型增加了L2正则项</li>
<li>Problem 2：为了验证正则的作用，在极小的数据集（1024）下反复迭代训练。观察到在没有正则的情况下，训练集的准确率很快收敛到了100%但是测试集准确率只有80.3%；增加正则项后训练集准确率迅速在85%处收敛，测试准确率86.3%。   </li>
<li>Problem 3：依然使用极小数据集，在隐层后面增加一个dropout层（但是要注意这个dropout层只有训练时用预测时不用），<code>keep_prob</code>设为0.5。</li>
<li>Problem 4：尝试更加复杂的网络（1024*256*64）,并使用<strong>learning rate decay</strong>, 效果显著。dropout和L2都没有取得预期的效果，可能是数据充足不存在太严重的过拟合现象吧。</li>
</ul>
<table>
<thead>
<tr>
<th>L2</th>
<th>Dropout</th>
<th>小数据集</th>
<th>大数据集</th>
<th>3层网络</th>
</tr>
</thead>
<tbody>
<tr>
<td>无</td>
<td>无</td>
<td>80.3%</td>
<td>91.1%</td>
<td>96.5%</td>
</tr>
<tr>
<td>有</td>
<td>无</td>
<td>86.3%</td>
<td>91%</td>
<td>96.3%</td>
</tr>
<tr>
<td>无</td>
<td>有</td>
<td>87.1%</td>
<td>90.8%</td>
<td>94.4%</td>
</tr>
<tr>
<td>有</td>
<td>有</td>
<td>88%</td>
<td>90.8%</td>
<td>94.4%</td>
</tr>
</tbody>
</table>
<h2 id="4-Convolution"><a href="#4-Convolution" class="headerlink" title="4.Convolution"></a>4.Convolution</h2><h3 id="Convolutional-Network"><a href="#Convolutional-Network" class="headerlink" title="Convolutional Network"></a>Convolutional Network</h3><p>首先我们要学习使用卷积函数，在TF中的调用函数为：<code>tf.nn.conv2d(input, filter, strides, padding)</code>，几个参数的含义依次是：</p>
<ul>
<li>input：输入图像，一个维度为[batch, height, width, in_channels]的tensor。      </li>
<li>filter：CNN中的卷积核函数，维度为[filter_height, filter_width, in_channels, out_channels]的tensor，这个tensor是我们需要学习的表  示。这里的in_channels应该与input中的in_channels一致。  </li>
<li>strides：做卷积时每一维的步长，是一个长度为4的一维向量。      </li>
<li>padding：填充方式，有”SAME”或”VALID”两种选择，区别在于卷积矩阵移动到图像之外时是否填充。<br>假如我们是有16个5*5的filter，out_chanel为16，步长设为[1,2,2,1]，SAME padding。则128*28*28*1的原始图像经过卷积处理后变为128*14*14*16。<br>这里给出的例子构造了conv(2*2)+conv(2*2)+fc(256)的网络结构，经过1000轮迭代后准确率为91.3。 <h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3>接下来，我们要学习一下<strong>pooling</strong>的使用，池化是一个subsampling的过程，相当于降维，常与1*1的卷积配合使用。调用函数为<code>tf.nn.max_pool(value, ksize, strides, padding)</code>。其中ksize为窗口大小，是一个长度为4的一维向量，strides和padding的含义同上。在使用了<code>16*conv(1*1)+max_pooling(2*2)+16*conv(1*1)+max_pooling(2*2)+fc(256)</code>后，准确率提高到92.3%（1000轮）</li>
</ul>
<h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>手写字母识别的最后一个任务是学习经典的CNN网络结构<a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="external">LeNet5</a>。<img src="../img/udacity-deep-learning/lenet-5.png" alt=""><br>其中的优化点没有仔细研究，照猫画虎构造了一个<code>8*conv(1*1)+max_pooling(2*2)+16*conv(1*1)+max_pooling(1*1)+fc(256)+fc(64)</code>的网络，增加L1正则后准确率达到94.8%（1000轮）,dropout依然没有取得正向效果。充分学习后准确率只有95.3%，不如裸的DNN，看来DL的调参学问果然是深不可测的玄学，以后有机会再继续调一下。   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">batch_size = <span class="number">256</span></div><div class="line">patch_size = <span class="number">5</span></div><div class="line">depth = [<span class="number">8</span>, <span class="number">16</span>]</div><div class="line">fc_dim = [<span class="number">256</span>, <span class="number">64</span>]</div><div class="line">lr = <span class="number">0.2</span></div><div class="line">beta = <span class="number">0.001</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight</span><span class="params">(shape)</span>:</span></div><div class="line">    w = tf.Variable(tf.truncated_normal(shape, stddev=<span class="number">0.1</span>))</div><div class="line">    b =  tf.Variable(tf.zeros([shape[<span class="number">-1</span>]]))</div><div class="line">    <span class="keyword">return</span> w,b</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(x, w)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, w, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pooling</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, keep_prob=<span class="number">1.0</span>)</span>:</span></div><div class="line">    <span class="comment"># 256*28*28*1</span></div><div class="line">    conv1 = tf.nn.relu(conv(x, w1) + b1)</div><div class="line">    <span class="comment"># 256*28*28*8</span></div><div class="line">    pool1 = max_pooling(conv1)</div><div class="line">    <span class="comment"># 256*14*14*8</span></div><div class="line">    conv2 = tf.nn.relu(conv(pool1, w2) + b2)</div><div class="line">    <span class="comment"># 256*14*14*16</span></div><div class="line">    pool2 = max_pooling(conv2)</div><div class="line">    <span class="comment"># 256*7*7*16</span></div><div class="line">    shape = pool2.get_shape().as_list()</div><div class="line">    flat = tf.reshape(pool2, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line">    <span class="comment"># 256*784</span></div><div class="line">    fc1 = tf.nn.relu(tf.matmul(flat, w3) + b3)</div><div class="line">    <span class="comment"># 256*256</span></div><div class="line">    fc2 = tf.nn.relu(tf.matmul(fc1, w4) + b4)</div><div class="line">    <span class="comment"># 256*64</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(fc2, w5) + b5</div><div class="line">    </div><div class="line">graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> graph.as_default():    </div><div class="line">  <span class="comment"># Input data.</span></div><div class="line">  x = tf.placeholder(tf.float32, shape=(batch_size, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</div><div class="line">  y = tf.placeholder(tf.float32, shape=(batch_size, <span class="number">10</span>))</div><div class="line">  valid = tf.constant(valid_dataset)</div><div class="line">  test = tf.constant(test_dataset)</div><div class="line">  <span class="comment"># Variables.</span></div><div class="line">  w1,b1 = weight([patch_size, patch_size, <span class="number">1</span>, depth[<span class="number">0</span>]])</div><div class="line">  w2,b2 = weight([patch_size, patch_size, depth[<span class="number">0</span>], depth[<span class="number">1</span>]])</div><div class="line">  w3,b3 = weight([<span class="number">7</span> * <span class="number">7</span> * depth[<span class="number">1</span>], fc_dim[<span class="number">0</span>]])</div><div class="line">  w4,b4 = weight([fc_dim[<span class="number">0</span>], fc_dim[<span class="number">1</span>]])</div><div class="line">  w5,b5 = weight([fc_dim[<span class="number">1</span>], <span class="number">10</span>])</div><div class="line">  <span class="comment">## optimizer</span></div><div class="line">  logits = model(x,<span class="number">0.5</span>)</div><div class="line">  l2 = tf.nn.l2_loss(w1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(b2) + \</div><div class="line">       tf.nn.l2_loss(w3) + tf.nn.l2_loss(b3) + tf.nn.l2_loss(w4) + tf.nn.l2_loss(b4) + \</div><div class="line">       tf.nn.l2_loss(w5) + tf.nn.l2_loss(b5)</div><div class="line">  loss = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits) + beta * l2)</div><div class="line">  global_step = tf.Variable(<span class="number">0</span>)</div><div class="line">  learning_rate = tf.train.exponential_decay(lr, global_step, <span class="number">100</span>, <span class="number">0.9</span>, staircase=<span class="keyword">True</span>)</div><div class="line">  optimizer = tf.train.GradientDescentOptimizer(learning_rate) \</div><div class="line">                .minimize(loss, global_step=global_step)</div><div class="line">  <span class="comment"># Predictions for the training, validation, and test data.</span></div><div class="line">  train_prediction = tf.nn.softmax(logits)</div><div class="line">  valid_prediction = tf.nn.softmax(model(valid))</div><div class="line">  test_prediction = tf.nn.softmax(model(test))   </div><div class="line">   </div><div class="line">num_steps = <span class="number">1001</span></div><div class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</div><div class="line">  tf.global_variables_initializer().run()</div><div class="line">  print(<span class="string">'Initialized'</span>)</div><div class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</div><div class="line">    offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</div><div class="line">    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]</div><div class="line">    batch_labels = train_labels[offset:(offset + batch_size), :]</div><div class="line">    feed_dict = &#123;x : batch_data, y : batch_labels&#125;</div><div class="line">    _, l, predictions = session.run(</div><div class="line">      [optimizer, loss, train_prediction], feed_dict=feed_dict)</div><div class="line">    <span class="keyword">if</span> (step % <span class="number">100</span> == <span class="number">0</span>):</div><div class="line">      print(<span class="string">'Minibatch loss at step %d: %f'</span> % (step, l))</div><div class="line">      print(<span class="string">'Minibatch accuracy: %.1f%%'</span> % accuracy(predictions, batch_labels))</div><div class="line">      print(<span class="string">'Validation accuracy: %.1f%%'</span> % accuracy(</div><div class="line">        valid_prediction.eval(), valid_labels))</div><div class="line">  print(<span class="string">'Test accuracy: %.1f%%'</span> % accuracy(test_prediction.eval(), test_labels))</div></pre></td></tr></table></figure>
<h2 id="5-word2vec"><a href="#5-word2vec" class="headerlink" title="5.word2vec"></a>5.word2vec</h2><p>使用<a href="http://mattmahoney.net/dc/textdata" target="_blank" rel="external">Text8</a>(维基百科预料)训练词的向量表示，已经给出了skip-gram算法的实现。   </p>
<ul>
<li>首先对数据进行处理，用默认编码代替低频词。  </li>
<li>根据<code>batch_size</code>, <code>num_skips</code>（每组数据采样的数量，随机不重复）, <code>skip_window</code>（每组数据目标单词两边的样本长度）进行采样。每组数据采样完成后窗口向后移一位，共移动<code>batch_size/num_skips</code>次。    </li>
<li>用128维的向量表示每个单词，共有N*128个embedding和weight，以及N各bias。  </li>
<li>由于这个多分类问题共有N个不同的label，因此训练时采用<code>sampled_softmax_loss</code>随机选取若干个（64）个负例计算loss。   </li>
<li>使用；训练过程中随机选取16个预测最近邻观察效果。<br><img src="../img/udacity-deep-learning/skip-gram-train.png" alt="">  </li>
<li>对于最终的embedding向量进行归一化，并使用sklearn中的<strong>T-SNE</strong>将高维向量映射到二维空间内观察相似度。<br><img src="../img/udacity-deep-learning/tsne-skip-gram.png" alt=""><br>众所周知，word2vec有skip-gram和CBOW（continuous bag of words）两种实现算法，其中skip-gram是给出target预测所在上下文的其他单词，而CBOW是根据上下文单词embedding的和来预测target。因此CBOW只需修改skip-gram的batch sampling方法，并修改输入向量维度和loss函数即可。 <blockquote>
<p>According to Mikolov:<br>Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.<br>CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words<br>This can get even a bit more complicated if you consider that there are two different ways how to train the models: the normalized hierarchical softmax, and the un-normalized negative sampling. Both work quite differently.  </p>
</blockquote>
</li>
</ul>
<p><img src="../img/udacity-deep-learning/cbow-train.png" alt=""><br>在相同的训练轮数下，skip-gram的loss约为3.2，而CBOW的loss约为2.3。由于CBOW使用了多个上下文向量的平均，因此得到的结果也更加平滑一些，在本数据集下效果也更好一些。<br><img src="../img/udacity-deep-learning/tsne-cbow.png" alt="">  </p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/udacity-deeplearning-assignment/" class="archive-article-date">
  	<time datetime="2017-03-09T14:30:38.000Z" itemprop="datePublished"><i class="icon-clock"></i>2017-03-09</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/online-course/">online course</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
  
    <a href="/coursera-microeconomics/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">UIUC《Microeconomics Principles》on Coursera</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>




<div class="share_jia">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">Share to: &nbsp; </span>
		<a class="jiathis_button_facebook"></a> 
	    <a class="jiathis_button_twitter"></a>
	    <a class="jiathis_button_plus"></a> 
	    <a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
    <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="//v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>






<div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="udacity-deeplearning-assignment" data-title="Udacity DeepLearning Assignment" data-url="http://sensirly.github.io/udacity-deeplearning-assignment/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"sensirly"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>





      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Eric Li
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: true,
		root: "/",
		innerArchive: true
	}
</script>

<script src="/./main.js"></script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/algorithm/" style="font-size: 12.5px;">algorithm</a> <a href="/tags/coursera/" style="font-size: 10px;">coursera</a> <a href="/tags/data-science/" style="font-size: 10px;">data science</a> <a href="/tags/economics/" style="font-size: 17.5px;">economics</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/online-course/" style="font-size: 20px;">online course</a> <a href="/tags/python/" style="font-size: 12.5px;">python</a> <a href="/tags/reading/" style="font-size: 15px;">reading</a> <a href="/tags/recommendation-system/" style="font-size: 12.5px;">recommendation system</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://blog.csdn.net/kksleric">我的ACM之路</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://sanmu-photography.lofter.com">我的LOFTER</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://www.hxytravel.com">海逍遥旅行</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://bqzhao.blogspot.com">乐田乐大人</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://www.jianshu.com/users/03474f176e0d">八汰sighby</a>
  	        
  	        </div>
  		
    	</section>
    

    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>